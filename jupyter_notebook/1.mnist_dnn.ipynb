{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __This is first basic tutorial for `tf.keras`.__\n",
    "\n",
    "#### We use MNIST handwrite dataset.\n",
    "#### And construct simple fully-connected(FC) layers.\n",
    "\n",
    "### __1. Load dependencies__\n",
    "\n",
    "First, we can load `tf.keras` and other python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version is 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2 ** 10)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"[INFO] Tensorflow version is {}\".format(tf.__version__))  # 1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we can load tensorflow eager model for better tensor management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total 3 steps in data pre-processing.\n",
    "\n",
    "* 1) Convert data type.\n",
    "* 2) Reshape data.\n",
    "* 3) Normalize data.\n",
    "\n",
    "We only perform the division of the dataset by 255 because dataset is 1 channel images and the distribution of data is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)     # convert data type\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))  # reshape: (28, 28) --> (784) (i.e., flatten 2D matrix to 1D vector)\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "x_train /= 255.  # normalization\n",
    "x_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.keras.utils` for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is (60000, 784)\n",
      "Shape of x_test is (10000, 784)\n",
      "Shape of y_train is (60000, 10)\n",
      "Shape of y_test is (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10).astype(np.float32)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10).astype(np.float32)\n",
    "\n",
    "print(\"Shape of x_train is {}\".format(np.shape(x_train)))\n",
    "print(\"Shape of x_test is {}\".format(np.shape(x_test)))\n",
    "print(\"Shape of y_train is {}\".format(np.shape(y_train)))\n",
    "print(\"Shape of y_test is {}\".format(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct tf.keras model\n",
    "\n",
    "We can construct tensorflow model by using `tf.keras.layers`.\n",
    "\n",
    "It's a lot easier than using an existing tensorflow, and the management of the tensor is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[INFO] Total parameters of model is 535818\n"
     ]
    }
   ],
   "source": [
    "# Build a tf.keras model\n",
    "inputs = tf.keras.Input(shape=(784,))\n",
    "\n",
    "fc_1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(inputs)\n",
    "fc_2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)(fc_1)\n",
    "probabilities = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(fc_2)\n",
    "\n",
    "#probabilities = tf.keras.layers.Dense(10, )(logits)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[probabilities])\n",
    "model.summary()\n",
    "print(\"[INFO] Total parameters of model is {}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to specify model's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset pipeline\n",
    "\n",
    "We use the `tf.data` tensor flow library to construct the link between dataset and model.\n",
    "\n",
    "Of course you can use Keras's basic `model.fit` method, but this example uses `tf.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 784), (?, 10)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_SIZE = 10000 \n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.shuffle(SHUFFLE_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note the batch dataset shape.\n",
    "\n",
    "Where shape must match the shape in `tf.summary` above.\n",
    "\n",
    "In this situation,? indicates __64__, the batch size we set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train model\n",
    "\n",
    "Finally, train our model for checking accuracy!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\t Loss: #0.9341843724250793\tAccuracy: #0.75\n",
      "Epoch #2\t Loss: #0.15050853788852692\tAccuracy: #0.9375\n",
      "Epoch #3\t Loss: #0.09951159358024597\tAccuracy: #0.9375\n",
      "Epoch #4\t Loss: #0.13128797709941864\tAccuracy: #0.9375\n",
      "Epoch #5\t Loss: #0.05193028226494789\tAccuracy: #1.0\n",
      "Epoch #6\t Loss: #0.12501972913742065\tAccuracy: #0.9375\n",
      "Epoch #7\t Loss: #0.0003079454763792455\tAccuracy: #1.0\n",
      "Epoch #8\t Loss: #0.7894665598869324\tAccuracy: #0.875\n",
      "Epoch #9\t Loss: #0.056634288281202316\tAccuracy: #0.9375\n",
      "Epoch #10\t Loss: #0.0029337259475141764\tAccuracy: #1.0\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "\n",
      "Test Model \t\t Loss: 0.150603\tAccuracy: 0.963300\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for (train_images, train_labels), (test_images, test_labels) in zip(dataset, test_dataset):\n",
    "        loss, acc = model.train_on_batch(train_images, train_labels)\n",
    "        test_loss, test_acc = model.test_on_batch(test_images, test_labels)\n",
    "    print('Epoch #{}\\t Loss: #{}\\tAccuracy: #{}'.format(epoch + 1, test_loss, test_acc))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('\\nTest Model \\t\\t Loss: %.6f\\tAccuracy: %.6f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not bad :)\n",
    "\n",
    "You can change `tf.keras` models and dataset configuration yourself.\n",
    "\n",
    "This concludes the **first** example. \n",
    "\n",
    "Next, let's design a tensorflow model that includes convolution using a __CIFAR__ dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
